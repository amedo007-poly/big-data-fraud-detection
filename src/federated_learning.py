"""
Federated Learning Simulation for Fraud Detection
==================================================
Simulates 3 banks training locally and aggregating models centrally
Uses FedAvg algorithm

Run: docker exec -it fraud-spark spark-submit /app/src/federated_learning.py
"""

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
import numpy as np
import json
import os

# ============================================================================
# CONFIGURATION
# ============================================================================

DATA_PATH = "/app/data/raw/creditcard.csv"
OUTPUT_PATH = "/app/outputs/metrics/federated_results.json"
NUM_ROUNDS = 5
NUM_CLIENTS = 3  # 3 Banks

# ============================================================================
# MAIN SIMULATION
# ============================================================================

def run_federated_learning():
    """
    Simule le Federated Learning avec 3 banques (clients)
    Chaque banque garde ses donnÃ©es privÃ©es et n'envoie que les gradients
    """
    
    print("=" * 70)
    print("   FEDERATED LEARNING - FRAUD DETECTION")
    print("   Simulation avec 3 Banques")
    print("=" * 70)
    
    # CrÃ©er la session Spark
    spark = SparkSession.builder \
        .appName("FederatedLearning_FraudDetection") \
        .master("local[*]") \
        .config("spark.driver.memory", "4g") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    # ========================================
    # 1. CHARGER LES DONNÃ‰ES
    # ========================================
    print("\n[1/5] Chargement des donnÃ©es...")
    
    df = spark.read.csv(DATA_PATH, header=True, inferSchema=True)
    total_count = df.count()
    fraud_count = df.filter(F.col("Class") == 1).count()
    
    print(f"     Total transactions: {total_count:,}")
    print(f"     Fraudes: {fraud_count:,} ({100*fraud_count/total_count:.2f}%)")
    
    # ========================================
    # 2. SIMULER LA PARTITION DES DONNÃ‰ES
    # ========================================
    print("\n[2/5] Partition des donnÃ©es entre 3 banques...")
    
    # Ajouter un ID pour partitionner
    df_indexed = df.withColumn("_idx", F.monotonically_increasing_id())
    
    # RÃ©partir entre 3 banques (simulation)
    # En rÃ©alitÃ©, chaque banque aurait ses propres donnÃ©es sÃ©parÃ©es
    bank_data = {
        "Banque_A": df_indexed.filter(F.col("_idx") % 3 == 0).drop("_idx"),
        "Banque_B": df_indexed.filter(F.col("_idx") % 3 == 1).drop("_idx"),
        "Banque_C": df_indexed.filter(F.col("_idx") % 3 == 2).drop("_idx")
    }
    
    for bank_name, bank_df in bank_data.items():
        count = bank_df.count()
        frauds = bank_df.filter(F.col("Class") == 1).count()
        print(f"     {bank_name}: {count:,} transactions ({frauds} fraudes)")
    
    # ========================================
    # 3. PRÃ‰PARER LES FEATURES
    # ========================================
    print("\n[3/5] PrÃ©paration des features...")
    
    feature_cols = [f"V{i}" for i in range(1, 29)] + ["Amount"]
    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features_raw")
    
    prepared_data = {}
    
    for bank_name, bank_df in bank_data.items():
        # Assembler features
        assembled = assembler.transform(bank_df)
        
        # Normaliser
        scaler = StandardScaler(inputCol="features_raw", outputCol="features", withStd=True, withMean=True)
        scaler_model = scaler.fit(assembled)
        scaled = scaler_model.transform(assembled)
        
        # Split train/test (80/20)
        train, test = scaled.randomSplit([0.8, 0.2], seed=42)
        
        prepared_data[bank_name] = {
            "train": train.cache(),
            "test": test.cache(),
            "train_count": train.count(),
            "test_count": test.count()
        }
        
        print(f"     {bank_name}: Train={prepared_data[bank_name]['train_count']:,}, Test={prepared_data[bank_name]['test_count']:,}")
    
    # ========================================
    # 4. FEDERATED LEARNING (FedAvg)
    # ========================================
    print("\n[4/5] DÃ©marrage Federated Learning (FedAvg)...")
    print("=" * 70)
    
    # Evaluators
    auc_evaluator = BinaryClassificationEvaluator(labelCol="Class", metricName="areaUnderROC")
    acc_evaluator = MulticlassClassificationEvaluator(labelCol="Class", predictionCol="prediction", metricName="accuracy")
    
    # Historique des rounds
    history = []
    
    # Variables pour stocker les poids globaux
    global_weights = None
    global_intercept = None
    
    for round_num in range(1, NUM_ROUNDS + 1):
        print(f"\n{'â”€'*25} ROUND {round_num}/{NUM_ROUNDS} {'â”€'*25}")
        
        round_results = {
            "round": round_num,
            "clients": [],
            "global_auc": 0.0
        }
        
        local_weights_list = []
        local_intercepts_list = []
        local_samples_list = []
        local_aucs = []
        
        # ========================================
        # Ã‰TAPE 1: EntraÃ®nement local sur chaque banque
        # ========================================
        for bank_name, data in prepared_data.items():
            # CrÃ©er et entraÃ®ner le modÃ¨le local
            lr = LogisticRegression(
                labelCol="Class",
                featuresCol="features",
                maxIter=10,  # Moins d'itÃ©rations par round
                regParam=0.01,
                elasticNetParam=0.8
            )
            
            # EntraÃ®ner sur les donnÃ©es locales
            local_model = lr.fit(data["train"])
            
            # Extraire les poids du modÃ¨le
            local_weights = local_model.coefficients.toArray()
            local_intercept = local_model.intercept
            
            # Ã‰valuer localement
            predictions = local_model.transform(data["test"])
            local_auc = auc_evaluator.evaluate(predictions)
            local_acc = acc_evaluator.evaluate(predictions)
            
            # Calculer precision/recall manuellement
            tp = predictions.filter((F.col("prediction") == 1) & (F.col("Class") == 1)).count()
            fp = predictions.filter((F.col("prediction") == 1) & (F.col("Class") == 0)).count()
            fn = predictions.filter((F.col("prediction") == 0) & (F.col("Class") == 1)).count()
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            
            # Stocker pour l'agrÃ©gation
            num_samples = data["train_count"]
            local_weights_list.append(local_weights)
            local_intercepts_list.append(local_intercept)
            local_samples_list.append(num_samples)
            local_aucs.append(local_auc)
            
            client_result = {
                "name": bank_name,
                "samples": num_samples,
                "auc": float(local_auc),
                "accuracy": float(local_acc),
                "precision": float(precision),
                "recall": float(recall)
            }
            round_results["clients"].append(client_result)
            
            print(f"   ðŸ“Š {bank_name}:")
            print(f"      AUC: {local_auc:.4f} | Accuracy: {local_acc:.4f}")
            print(f"      Precision: {precision:.4f} | Recall: {recall:.4f}")
            print(f"      Samples: {num_samples:,}")
        
        # ========================================
        # Ã‰TAPE 2: AgrÃ©gation centrale (FedAvg)
        # ========================================
        print(f"\n   ðŸ”„ AGRÃ‰GATION CENTRALE (FedAvg):")
        
        total_samples = sum(local_samples_list)
        
        # Moyenne pondÃ©rÃ©e des poids
        global_weights = np.zeros_like(local_weights_list[0])
        global_intercept = 0.0
        
        for i, (weights, intercept, n_samples) in enumerate(zip(
            local_weights_list, local_intercepts_list, local_samples_list
        )):
            weight_factor = n_samples / total_samples
            global_weights += weight_factor * weights
            global_intercept += weight_factor * intercept
        
        # AUC global (moyenne pondÃ©rÃ©e)
        global_auc = sum(
            auc * n / total_samples 
            for auc, n in zip(local_aucs, local_samples_list)
        )
        
        round_results["global_auc"] = float(global_auc)
        history.append(round_results)
        
        print(f"      Poids agrÃ©gÃ©s de {NUM_CLIENTS} clients")
        print(f"      Total samples: {total_samples:,}")
        print(f"      âœ… AUC Global: {global_auc:.4f}")
    
    # ========================================
    # 5. RÃ‰SULTATS FINAUX
    # ========================================
    print("\n" + "=" * 70)
    print("   RÃ‰SULTATS FINAUX - FEDERATED LEARNING")
    print("=" * 70)
    
    final_round = history[-1]
    
    print(f"\n   ðŸ“ˆ Performance aprÃ¨s {NUM_ROUNDS} rounds:")
    print(f"      AUC Global: {final_round['global_auc']:.4f}")
    print(f"\n   ðŸ“Š Performance par banque (Round final):")
    
    for client in final_round["clients"]:
        print(f"      {client['name']}:")
        print(f"         AUC: {client['auc']:.4f}")
        print(f"         Precision: {client['precision']:.4f}")
        print(f"         Recall: {client['recall']:.4f}")
    
    # ========================================
    # 6. SAUVEGARDER LES RÃ‰SULTATS
    # ========================================
    results = {
        "algorithm": "FedAvg",
        "description": "Federated Averaging pour dÃ©tection de fraude",
        "num_rounds": NUM_ROUNDS,
        "num_clients": NUM_CLIENTS,
        "total_transactions": total_count,
        "total_frauds": fraud_count,
        "final_global_auc": float(final_round["global_auc"]),
        "history": history,
        "privacy_preserved": True,
        "data_centralization": False
    }
    
    # CrÃ©er le dossier si nÃ©cessaire
    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
    
    with open(OUTPUT_PATH, "w") as f:
        json.dump(results, f, indent=2)
    
    print(f"\n   ðŸ’¾ RÃ©sultats sauvegardÃ©s: {OUTPUT_PATH}")
    
    # ========================================
    # AVANTAGES DU FEDERATED LEARNING
    # ========================================
    print("\n" + "=" * 70)
    print("   AVANTAGES DU FEDERATED LEARNING")
    print("=" * 70)
    print("""
   âœ… ConfidentialitÃ©: Les donnÃ©es restent chez chaque banque
   âœ… ConformitÃ© RGPD: Pas de centralisation des donnÃ©es sensibles
   âœ… ScalabilitÃ©: Ajout facile de nouvelles banques partenaires
   âœ… Robustesse: ModÃ¨le plus gÃ©nÃ©ralisable (donnÃ©es diverses)
   âœ… SÃ©curitÃ©: Seuls les gradients chiffrÃ©s sont transmis
    """)
    
    spark.stop()
    
    return results


# ============================================================================
# ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    results = run_federated_learning()
    print("\nâœ… Simulation Federated Learning terminÃ©e!")
