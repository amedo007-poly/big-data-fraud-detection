# ğŸš€ Guide d'IntÃ©gration Azure & Federated Learning

## Table des MatiÃ¨res
1. [Azure Databricks - DÃ©ploiement](#1-azure-databricks---dÃ©ploiement)
2. [Federated Learning - ImplÃ©mentation](#2-federated-learning---implÃ©mentation)
3. [Architecture ComplÃ¨te](#3-architecture-complÃ¨te)

---

## 1. Azure Databricks - DÃ©ploiement

### 1.1 PrÃ©requis

```bash
# Installer Azure CLI
# Windows: winget install Microsoft.AzureCLI
# Ou tÃ©lÃ©charger: https://aka.ms/installazurecliwindows

# Se connecter Ã  Azure
az login

# VÃ©rifier la souscription
az account show
```

### 1.2 CrÃ©er les Ressources Azure

```bash
# Variables
RESOURCE_GROUP="fraud-detection-rg"
LOCATION="westeurope"
DATABRICKS_WORKSPACE="fraud-databricks-ws"
STORAGE_ACCOUNT="frauddatastorage"

# 1. CrÃ©er le Resource Group
az group create --name $RESOURCE_GROUP --location $LOCATION

# 2. CrÃ©er le Storage Account (Data Lake Gen2)
az storage account create \
    --name $STORAGE_ACCOUNT \
    --resource-group $RESOURCE_GROUP \
    --location $LOCATION \
    --sku Standard_LRS \
    --kind StorageV2 \
    --hierarchical-namespace true

# 3. CrÃ©er le conteneur pour les donnÃ©es
az storage container create \
    --name fraud-data \
    --account-name $STORAGE_ACCOUNT

# 4. CrÃ©er Azure Databricks Workspace
az databricks workspace create \
    --resource-group $RESOURCE_GROUP \
    --name $DATABRICKS_WORKSPACE \
    --location $LOCATION \
    --sku standard
```

### 1.3 Uploader les DonnÃ©es vers Azure

```bash
# Obtenir la clÃ© du storage
STORAGE_KEY=$(az storage account keys list \
    --account-name $STORAGE_ACCOUNT \
    --resource-group $RESOURCE_GROUP \
    --query '[0].value' -o tsv)

# Uploader le dataset
az storage blob upload \
    --account-name $STORAGE_ACCOUNT \
    --account-key $STORAGE_KEY \
    --container-name fraud-data \
    --file data/raw/creditcard.csv \
    --name raw/creditcard.csv
```

### 1.4 Configurer Databricks

1. **AccÃ©der au workspace**: Portal Azure â†’ Databricks â†’ Launch Workspace

2. **CrÃ©er un Cluster**:
```python
# Dans Databricks, crÃ©er un notebook et configurer:
cluster_config = {
    "cluster_name": "fraud-detection-cluster",
    "spark_version": "13.3.x-scala2.12",
    "node_type_id": "Standard_DS3_v2",
    "autoscale": {
        "min_workers": 2,
        "max_workers": 8
    },
    "spark_conf": {
        "spark.sql.adaptive.enabled": "true",
        "spark.sql.adaptive.coalescePartitions.enabled": "true"
    }
}
```

3. **Monter le Storage**:
```python
# Dans un notebook Databricks
storage_account = "frauddatastorage"
container = "fraud-data"
storage_key = dbutils.secrets.get(scope="fraud-scope", key="storage-key")

dbutils.fs.mount(
    source=f"wasbs://{container}@{storage_account}.blob.core.windows.net",
    mount_point="/mnt/fraud-data",
    extra_configs={f"fs.azure.account.key.{storage_account}.blob.core.windows.net": storage_key}
)

# VÃ©rifier
display(dbutils.fs.ls("/mnt/fraud-data/raw/"))
```

### 1.5 ExÃ©cuter le Pipeline sur Databricks

```python
# Notebook: fraud_detection_pipeline.py

from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import BinaryClassificationEvaluator
import mlflow

# MLflow tracking
mlflow.set_experiment("/fraud-detection")

# Charger les donnÃ©es depuis Azure Storage
df = spark.read.csv("/mnt/fraud-data/raw/creditcard.csv", header=True, inferSchema=True)

print(f"Total transactions: {df.count()}")
print(f"Fraudes: {df.filter(df.Class == 1).count()}")

# Feature Engineering
feature_cols = [f"V{i}" for i in range(1, 29)] + ["Amount"]
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features_raw")
scaler = StandardScaler(inputCol="features_raw", outputCol="features")

# Train/Test Split
train, test = df.randomSplit([0.8, 0.2], seed=42)

# Pipeline avec MLflow tracking
with mlflow.start_run(run_name="RandomForest_Azure"):
    # Transformer
    train_assembled = assembler.transform(train)
    scaler_model = scaler.fit(train_assembled)
    train_scaled = scaler_model.transform(train_assembled)
    
    # EntraÃ®ner
    rf = RandomForestClassifier(
        labelCol="Class",
        featuresCol="features",
        numTrees=100,
        maxDepth=10
    )
    model = rf.fit(train_scaled)
    
    # Ã‰valuer
    test_assembled = assembler.transform(test)
    test_scaled = scaler_model.transform(test_assembled)
    predictions = model.transform(test_scaled)
    
    evaluator = BinaryClassificationEvaluator(labelCol="Class", metricName="areaUnderROC")
    auc = evaluator.evaluate(predictions)
    
    # Log metrics
    mlflow.log_param("num_trees", 100)
    mlflow.log_param("max_depth", 10)
    mlflow.log_metric("auc", auc)
    mlflow.spark.log_model(model, "model")
    
    print(f"AUC-ROC: {auc:.4f}")

# Sauvegarder le modÃ¨le
model.write().overwrite().save("/mnt/fraud-data/models/random_forest_v1")
```

### 1.6 Streaming avec Azure Event Hubs

```python
# Configuration Event Hubs
ehConf = {
    'eventhubs.connectionString': 
        sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(
            "Endpoint=sb://fraud-eventhub.servicebus.windows.net/;SharedAccessKeyName=listen;SharedAccessKey=xxx;EntityPath=transactions"
        )
}

# Lire le stream
stream_df = spark.readStream \
    .format("eventhubs") \
    .options(**ehConf) \
    .load()

# Appliquer le modÃ¨le en temps rÃ©el
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, DoubleType

schema = StructType([...])  # DÃ©finir le schÃ©ma

parsed_stream = stream_df \
    .select(from_json(col("body").cast("string"), schema).alias("data")) \
    .select("data.*")

# Scoring
scored_stream = model.transform(parsed_stream)

# Ã‰crire les alertes
scored_stream.filter(col("prediction") == 1) \
    .writeStream \
    .format("delta") \
    .outputMode("append") \
    .option("checkpointLocation", "/mnt/fraud-data/checkpoints/alerts") \
    .start("/mnt/fraud-data/alerts")
```

---

## 2. Federated Learning - ImplÃ©mentation

### 2.1 Architecture Federated Learning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AGGREGATEUR CENTRAL                       â”‚
â”‚                  (Azure Functions / VM)                      â”‚
â”‚                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚   â”‚ Global Modelâ”‚  â”‚  FedAvg     â”‚  â”‚  Model      â”‚        â”‚
â”‚   â”‚  Weights    â”‚  â”‚  Algorithm  â”‚  â”‚  Registry   â”‚        â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚                   â”‚
        â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BANQUE A    â”‚   â”‚   BANQUE B    â”‚   â”‚   BANQUE C    â”‚
â”‚   (Client 1)  â”‚   â”‚   (Client 2)  â”‚   â”‚   (Client 3)  â”‚
â”‚               â”‚   â”‚               â”‚   â”‚               â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚Local Data â”‚ â”‚   â”‚ â”‚Local Data â”‚ â”‚   â”‚ â”‚Local Data â”‚ â”‚
â”‚ â”‚(Private)  â”‚ â”‚   â”‚ â”‚(Private)  â”‚ â”‚   â”‚ â”‚(Private)  â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚       â”‚   â”‚       â”‚       â”‚   â”‚       â”‚       â”‚
â”‚       â–¼       â”‚   â”‚       â–¼       â”‚   â”‚       â–¼       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚Local Modelâ”‚ â”‚   â”‚ â”‚Local Modelâ”‚ â”‚   â”‚ â”‚Local Modelâ”‚ â”‚
â”‚ â”‚Training   â”‚ â”‚   â”‚ â”‚Training   â”‚ â”‚   â”‚ â”‚Training   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚       â”‚       â”‚   â”‚       â”‚       â”‚   â”‚       â”‚       â”‚
â”‚       â–¼       â”‚   â”‚       â–¼       â”‚   â”‚       â–¼       â”‚
â”‚  Gradients    â”‚   â”‚  Gradients    â”‚   â”‚  Gradients    â”‚
â”‚  (encrypted)  â”‚   â”‚  (encrypted)  â”‚   â”‚  (encrypted)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â”‚                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                    â–¼ Upload Gradients â–¼
```

### 2.2 ImplÃ©mentation avec PySpark + Flower

CrÃ©er le fichier `src/federated_learning.py`:

```python
"""
Federated Learning pour DÃ©tection de Fraude
Utilise Flower (flwr) pour l'orchestration
"""

import flwr as fl
import numpy as np
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from typing import Dict, List, Tuple
import json


# ============================================================================
# CLIENT FEDERATED (Chaque banque)
# ============================================================================

class FraudDetectionClient(fl.client.NumPyClient):
    """Client Federated Learning pour une banque"""
    
    def __init__(self, client_id: str, data_path: str):
        self.client_id = client_id
        self.spark = SparkSession.builder \
            .appName(f"FederatedClient_{client_id}") \
            .getOrCreate()
        
        # Charger les donnÃ©es locales (privÃ©es)
        self.df = self.spark.read.csv(data_path, header=True, inferSchema=True)
        self.feature_cols = [f"V{i}" for i in range(1, 29)] + ["Amount"]
        
        # PrÃ©parer les features
        self._prepare_data()
        
        # ModÃ¨le local
        self.model = None
        
    def _prepare_data(self):
        """PrÃ©pare les donnÃ©es pour l'entraÃ®nement"""
        assembler = VectorAssembler(
            inputCols=self.feature_cols, 
            outputCol="features_raw"
        )
        self.df = assembler.transform(self.df)
        
        scaler = StandardScaler(inputCol="features_raw", outputCol="features")
        scaler_model = scaler.fit(self.df)
        self.df = scaler_model.transform(self.df)
        
        # Split train/test
        self.train_df, self.test_df = self.df.randomSplit([0.8, 0.2], seed=42)
        
    def get_parameters(self, config) -> List[np.ndarray]:
        """Retourne les paramÃ¨tres du modÃ¨le local"""
        if self.model is None:
            # Initialiser avec des poids alÃ©atoires
            return [np.random.randn(29).astype(np.float32)]
        
        # Extraire les coefficients du modÃ¨le Spark
        coefficients = self.model.coefficients.toArray()
        intercept = np.array([self.model.intercept])
        return [coefficients, intercept]
    
    def set_parameters(self, parameters: List[np.ndarray]):
        """Met Ã  jour les paramÃ¨tres avec le modÃ¨le global"""
        # Les paramÃ¨tres seront utilisÃ©s pour initialiser le prochain entraÃ®nement
        self.global_weights = parameters
        
    def fit(self, parameters: List[np.ndarray], config: Dict) -> Tuple[List[np.ndarray], int, Dict]:
        """EntraÃ®ne le modÃ¨le sur les donnÃ©es locales"""
        print(f"[Client {self.client_id}] EntraÃ®nement local...")
        
        # Mettre Ã  jour avec les poids globaux
        self.set_parameters(parameters)
        
        # EntraÃ®ner le modÃ¨le local
        lr = LogisticRegression(
            labelCol="Class",
            featuresCol="features",
            maxIter=10,  # Moins d'itÃ©rations pour federated
            regParam=0.01
        )
        self.model = lr.fit(self.train_df)
        
        # Retourner les nouveaux paramÃ¨tres
        num_samples = self.train_df.count()
        return self.get_parameters(config), num_samples, {}
    
    def evaluate(self, parameters: List[np.ndarray], config: Dict) -> Tuple[float, int, Dict]:
        """Ã‰value le modÃ¨le sur les donnÃ©es locales"""
        self.set_parameters(parameters)
        
        if self.model is None:
            return 0.0, 0, {"auc": 0.0}
        
        # PrÃ©dictions
        predictions = self.model.transform(self.test_df)
        
        # Ã‰valuation
        evaluator = BinaryClassificationEvaluator(
            labelCol="Class",
            metricName="areaUnderROC"
        )
        auc = evaluator.evaluate(predictions)
        
        num_samples = self.test_df.count()
        return float(1 - auc), num_samples, {"auc": float(auc)}


# ============================================================================
# SERVEUR FEDERATED (Aggregateur Central)
# ============================================================================

def weighted_average(metrics: List[Tuple[int, Dict]]) -> Dict:
    """Calcule la moyenne pondÃ©rÃ©e des mÃ©triques"""
    total_samples = sum([num_samples for num_samples, _ in metrics])
    
    weighted_auc = sum([
        num_samples * m["auc"] for num_samples, m in metrics
    ]) / total_samples
    
    return {"auc": weighted_auc}


class FedAvgStrategy(fl.server.strategy.FedAvg):
    """StratÃ©gie FedAvg personnalisÃ©e pour la dÃ©tection de fraude"""
    
    def __init__(self, min_clients: int = 2, min_fit_clients: int = 2):
        super().__init__(
            min_available_clients=min_clients,
            min_fit_clients=min_fit_clients,
            min_evaluate_clients=min_fit_clients,
            evaluate_metrics_aggregation_fn=weighted_average
        )
        
    def aggregate_fit(self, server_round, results, failures):
        """AgrÃ¨ge les modÃ¨les avec FedAvg"""
        print(f"\n[Server] Round {server_round}: AgrÃ©gation de {len(results)} modÃ¨les")
        
        # Appeler l'agrÃ©gation standard FedAvg
        aggregated = super().aggregate_fit(server_round, results, failures)
        
        if aggregated is not None:
            print(f"[Server] ModÃ¨le global mis Ã  jour")
            
        return aggregated


# ============================================================================
# SIMULATION LOCALE (3 Banques)
# ============================================================================

def simulate_federated_learning():
    """
    Simule le Federated Learning avec 3 clients (banques)
    Pour une vraie implÃ©mentation, chaque client serait sur une machine sÃ©parÃ©e
    """
    from pyspark.sql import SparkSession
    import pandas as pd
    
    print("=" * 60)
    print("FEDERATED LEARNING - SIMULATION")
    print("=" * 60)
    
    # CrÃ©er une session Spark
    spark = SparkSession.builder \
        .appName("FederatedSimulation") \
        .master("local[*]") \
        .getOrCreate()
    
    # Charger le dataset complet
    df = spark.read.csv("/app/data/raw/creditcard.csv", header=True, inferSchema=True)
    total = df.count()
    print(f"\nDataset total: {total} transactions")
    
    # Simuler 3 banques avec des partitions diffÃ©rentes
    # En rÃ©alitÃ©, chaque banque aurait ses propres donnÃ©es
    df_with_id = df.withColumn("row_id", F.monotonically_increasing_id())
    
    bank_a = df_with_id.filter(F.col("row_id") % 3 == 0)
    bank_b = df_with_id.filter(F.col("row_id") % 3 == 1)
    bank_c = df_with_id.filter(F.col("row_id") % 3 == 2)
    
    print(f"\nBanque A: {bank_a.count()} transactions")
    print(f"Banque B: {bank_b.count()} transactions")
    print(f"Banque C: {bank_c.count()} transactions")
    
    # Feature Engineering
    feature_cols = [f"V{i}" for i in range(1, 29)] + ["Amount"]
    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features_raw")
    scaler = StandardScaler(inputCol="features_raw", outputCol="features")
    
    def prepare_bank_data(bank_df, bank_name):
        assembled = assembler.transform(bank_df)
        scaler_model = scaler.fit(assembled)
        scaled = scaler_model.transform(assembled)
        train, test = scaled.randomSplit([0.8, 0.2], seed=42)
        print(f"  {bank_name}: Train={train.count()}, Test={test.count()}")
        return train, test, scaler_model
    
    print("\nPrÃ©paration des donnÃ©es par banque:")
    train_a, test_a, _ = prepare_bank_data(bank_a, "Banque A")
    train_b, test_b, _ = prepare_bank_data(bank_b, "Banque B")
    train_c, test_c, _ = prepare_bank_data(bank_c, "Banque C")
    
    # ========================================
    # FEDERATED LEARNING SIMULATION
    # ========================================
    
    from pyspark.ml.classification import LogisticRegression
    from pyspark.ml.evaluation import BinaryClassificationEvaluator
    import numpy as np
    
    NUM_ROUNDS = 5
    
    # Initialiser les poids globaux
    global_weights = None
    
    evaluator = BinaryClassificationEvaluator(labelCol="Class", metricName="areaUnderROC")
    
    print("\n" + "=" * 60)
    print("DÃ‰MARRAGE FEDERATED LEARNING")
    print("=" * 60)
    
    for round_num in range(1, NUM_ROUNDS + 1):
        print(f"\n{'='*20} ROUND {round_num} {'='*20}")
        
        local_weights = []
        local_samples = []
        local_aucs = []
        
        # Chaque banque entraÃ®ne localement
        for bank_name, train_df, test_df in [
            ("Banque A", train_a, test_a),
            ("Banque B", train_b, test_b),
            ("Banque C", train_c, test_c)
        ]:
            # EntraÃ®nement local
            lr = LogisticRegression(
                labelCol="Class",
                featuresCol="features",
                maxIter=10,
                regParam=0.01
            )
            model = lr.fit(train_df)
            
            # Extraire les poids
            weights = model.coefficients.toArray()
            intercept = model.intercept
            
            # Ã‰valuer localement
            predictions = model.transform(test_df)
            auc = evaluator.evaluate(predictions)
            
            num_samples = train_df.count()
            local_weights.append((weights, intercept, num_samples))
            local_samples.append(num_samples)
            local_aucs.append(auc)
            
            print(f"  {bank_name}: AUC={auc:.4f}, Samples={num_samples}")
        
        # ========================================
        # FEDAVG: AgrÃ©gation centrale
        # ========================================
        total_samples = sum(local_samples)
        
        # Moyenne pondÃ©rÃ©e des poids
        avg_weights = np.zeros_like(local_weights[0][0])
        avg_intercept = 0.0
        
        for weights, intercept, n_samples in local_weights:
            weight_factor = n_samples / total_samples
            avg_weights += weight_factor * weights
            avg_intercept += weight_factor * intercept
        
        global_weights = (avg_weights, avg_intercept)
        
        # Calculer l'AUC global (moyenne pondÃ©rÃ©e)
        global_auc = sum(auc * n / total_samples 
                       for auc, n in zip(local_aucs, local_samples))
        
        print(f"\n  [AGGREGATEUR] AUC Global: {global_auc:.4f}")
    
    print("\n" + "=" * 60)
    print("FEDERATED LEARNING TERMINÃ‰")
    print(f"AUC Final: {global_auc:.4f}")
    print("=" * 60)
    
    # Sauvegarder les rÃ©sultats
    results = {
        "algorithm": "FedAvg",
        "num_rounds": NUM_ROUNDS,
        "num_clients": 3,
        "final_auc": float(global_auc),
        "clients": [
            {"name": "Banque A", "samples": int(local_samples[0]), "auc": float(local_aucs[0])},
            {"name": "Banque B", "samples": int(local_samples[1]), "auc": float(local_aucs[1])},
            {"name": "Banque C", "samples": int(local_samples[2]), "auc": float(local_aucs[2])}
        ]
    }
    
    with open("/app/outputs/metrics/federated_results.json", "w") as f:
        json.dump(results, f, indent=2)
    
    print(f"\nRÃ©sultats sauvegardÃ©s: /app/outputs/metrics/federated_results.json")
    
    return results


if __name__ == "__main__":
    from pyspark.sql import functions as F
    simulate_federated_learning()
```

### 2.3 ExÃ©cuter la Simulation

```bash
# Dans le conteneur Docker
docker exec -it fraud-spark spark-submit \
    --master local[*] \
    /app/src/federated_learning.py
```

### 2.4 DÃ©ploiement RÃ©el avec Azure

Pour un dÃ©ploiement en production:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     AZURE ARCHITECTURE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Azure Functions (Aggregateur)               â”‚   â”‚
â”‚  â”‚  - ReÃ§oit les gradients chiffrÃ©s                        â”‚   â”‚
â”‚  â”‚  - ExÃ©cute FedAvg                                       â”‚   â”‚
â”‚  â”‚  - Distribue le modÃ¨le global                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â”‚                                  â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚              â”‚               â”‚               â”‚                  â”‚
â”‚              â–¼               â–¼               â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Azure Event  â”‚ â”‚  Azure Event  â”‚ â”‚  Azure Event  â”‚        â”‚
â”‚  â”‚  Hubs (A)     â”‚ â”‚  Hubs (B)     â”‚ â”‚  Hubs (C)     â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚          â”‚                 â”‚                 â”‚                  â”‚
â”‚          â–¼                 â–¼                 â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ Databricks A  â”‚ â”‚ Databricks B  â”‚ â”‚ Databricks C  â”‚        â”‚
â”‚  â”‚ (Banque A)    â”‚ â”‚ (Banque B)    â”‚ â”‚ (Banque C)    â”‚        â”‚
â”‚  â”‚               â”‚ â”‚               â”‚ â”‚               â”‚        â”‚
â”‚  â”‚ Private Data  â”‚ â”‚ Private Data  â”‚ â”‚ Private Data  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Azure Key Vault                             â”‚   â”‚
â”‚  â”‚  - ClÃ©s de chiffrement                                  â”‚   â”‚
â”‚  â”‚  - Secrets pour communication sÃ©curisÃ©e                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Architecture ComplÃ¨te

### 3.1 Commandes de DÃ©ploiement Rapide

```bash
# 1. CrÃ©er toutes les ressources Azure
cd azure/
chmod +x deploy.sh
./deploy.sh

# 2. Configurer Databricks
az databricks workspace show --name fraud-databricks-ws --resource-group fraud-detection-rg

# 3. DÃ©ployer le modÃ¨le
databricks jobs create --json-file jobs/fraud_training_job.json
databricks jobs run-now --job-id <JOB_ID>
```

### 3.2 CoÃ»ts EstimÃ©s

| Service | Usage | CoÃ»t/Mois |
|---------|-------|-----------|
| Azure Databricks | Standard, 2-8 workers | ~$120 |
| Data Lake Gen2 | 100 GB | ~$5 |
| Event Hubs | Basic, 1M events | ~$15 |
| Azure Functions | Consumption | ~$5 |
| Key Vault | 10K operations | ~$3 |
| **Total** | | **~$150/mois** |

### 3.3 Checklist de DÃ©ploiement

- [ ] CrÃ©er Resource Group Azure
- [ ] DÃ©ployer Storage Account (Data Lake Gen2)
- [ ] CrÃ©er Azure Databricks Workspace
- [ ] Configurer Event Hubs pour streaming
- [ ] Uploader les donnÃ©es initiales
- [ ] CrÃ©er et lancer le cluster Databricks
- [ ] DÃ©ployer le pipeline MLlib
- [ ] Configurer le monitoring (Azure Monitor)
- [ ] (Optionnel) DÃ©ployer Federated Learning

---

## ğŸ“š Ressources

- [Azure Databricks Documentation](https://docs.microsoft.com/azure/databricks/)
- [Flower Federated Learning](https://flower.dev/)
- [PySpark MLlib](https://spark.apache.org/docs/latest/ml-guide.html)
- [Azure Event Hubs + Spark](https://docs.microsoft.com/azure/event-hubs/event-hubs-spark-connector)
