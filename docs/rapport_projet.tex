\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{tocloft}

% Page setup
\geometry{margin=2.5cm}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.5em}

% Colors
\definecolor{primaryblue}{RGB}{0, 82, 147}
\definecolor{secondaryred}{RGB}{204, 0, 0}
\definecolor{codegray}{RGB}{245, 245, 245}
\definecolor{codegreen}{RGB}{0, 128, 0}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=primaryblue,
    filecolor=primaryblue,
    urlcolor=primaryblue,
}

% Code listing setup
\lstset{
    backgroundcolor=\color{codegray},
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    language=Python,
    keywordstyle=\color{primaryblue}\bfseries,
    commentstyle=\color{codegreen},
    stringstyle=\color{secondaryred},
    numbers=left,
    numberstyle=\tiny\color{gray},
    tabsize=4
}

% Header/Footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\textcolor{primaryblue}{Big Data - Projet Final}}
\fancyhead[R]{\textcolor{primaryblue}{Détection de Fraude}}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0.5pt}

% Title formatting
\titleformat{\section}{\Large\bfseries\color{primaryblue}}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries\color{primaryblue!80}}{\thesubsection}{1em}{}

% ============================================================================
% DOCUMENT START
% ============================================================================

\begin{document}

% Title Page
\begin{titlepage}
    \centering
    \vspace*{2cm}
    
    {\Huge\bfseries\textcolor{primaryblue}{Projet Big Data}}\\[0.5cm]
    {\LARGE\textcolor{primaryblue}{Module Clôture}}\\[2cm]
    
    \rule{\textwidth}{1.5pt}\\[0.5cm]
    {\Huge\bfseries Détection de Fraude Bancaire}\\[0.3cm]
    {\Large avec Apache Spark et Machine Learning}\\[0.5cm]
    \rule{\textwidth}{1.5pt}\\[2cm]
    
    \includegraphics[width=0.3\textwidth]{C:/Users/ahmed/OneDrive/Desktop/Everything/BIG Data Hadoop/Final Project/big-data-fraud-project/screenshots/spark-logo.png}\\[1cm] % Add logo if available
    
    {\Large\textbf{Technologies utilisées:}}\\[0.5cm]
    {\large Apache Spark $\bullet$ Spark SQL $\bullet$ MLlib $\bullet$ Spark Streaming $\bullet$ Grafana}\\[1.5cm]
    
    \vfill
    
    {\Large\textbf{Équipe:}}\\[0.3cm]
    {\large Ahmed Dinari $\bullet$ Bilel Samaali $\bullet$ Anas Belhouichet}\\[0.5cm]
    {\large\textbf{Date:} \today}\\[0.3cm]
    {\large\textbf{Module:} Big Data Hadoop}
    
\end{titlepage}

% Team Contributions Page
\newpage
\section*{Contributions de l'Équipe}
\addcontentsline{toc}{section}{Contributions de l'Équipe}

\begin{table}[H]
\centering
\begin{tabular}{@{}p{4cm}p{4cm}p{6cm}@{}}
\toprule
\textbf{Membre} & \textbf{Rôle} & \textbf{Contributions} \\
\midrule
Ahmed Dinari & Lead Developer \& ML Engineer & Architecture pipeline, Spark Core, Modèles MLlib, Configuration Docker \\
Bilel Samaali & Data Engineer & Spark SQL Analytics, Nettoyage données, Feature Engineering \\
Anas Belhouichet & Visualisation \& Documentation & Dashboard Grafana, Rapport LaTeX, Présentation Beamer \\
\bottomrule
\end{tabular}
\caption{Répartition du travail}
\end{table}

% Table of Contents
\tableofcontents
\newpage

% ============================================================================
% INTRODUCTION
% ============================================================================

\section{Introduction}

\subsection{Contexte du Projet}
La détection de fraude bancaire est un enjeu majeur dans le secteur financier. Avec des millions de transactions effectuées quotidiennement, il est crucial de pouvoir identifier rapidement les transactions frauduleuses tout en minimisant les faux positifs.

Ce projet met en œuvre un pipeline Big Data complet pour la détection de fraude en temps réel, utilisant l'écosystème Apache Spark.

\subsection{Objectifs}
\begin{itemize}
    \item \textbf{Ingestion et nettoyage} des données avec Spark SQL
    \item \textbf{Analyse exploratoire} avec des KPIs pertinents
    \item \textbf{Machine Learning} avec MLlib (RandomForest, Logistic Regression)
    \item \textbf{Streaming temps réel} pour la détection continue
    \item \textbf{Visualisation} via dashboard Grafana
\end{itemize}

\subsection{Dataset}
\textbf{Credit Card Fraud Detection Dataset} (Kaggle)
\begin{itemize}
    \item \textbf{Source:} \url{https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud}
    \item \textbf{Transactions:} 284,807
    \item \textbf{Features:} 30 (V1-V28 transformées PCA + Time + Amount)
    \item \textbf{Classe:} Binaire (0 = Normal, 1 = Fraude)
    \item \textbf{Distribution:} 99.83\% Normal, 0.17\% Fraude
\end{itemize}

% ============================================================================
% ARCHITECTURE
% ============================================================================

\section{Architecture du Pipeline}

\subsection{Vue d'Ensemble}

\begin{figure}[H]
    \centering
    \fbox{\parbox{0.9\textwidth}{
        \centering
        \textbf{ARCHITECTURE DU PIPELINE BIG DATA}\\[0.5cm]
        \texttt{CSV Data} $\rightarrow$ \texttt{Spark Core} $\rightarrow$ \texttt{Spark SQL} $\rightarrow$ \texttt{MLlib} $\rightarrow$ \texttt{Streaming} $\rightarrow$ \texttt{Grafana}
    }}
    \caption{Pipeline de traitement des données}
\end{figure}

\subsection{Composants}

\begin{table}[H]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Composant} & \textbf{Technologie} & \textbf{Rôle} \\
\midrule
Ingestion & Spark Core & Chargement CSV \\
Nettoyage & Spark SQL & Transformation, filtrage \\
Analytics & Spark SQL & KPIs, agrégations \\
ML Model & MLlib & Classification fraude \\
Streaming & Structured Streaming & Temps réel \\
Visualisation & Grafana & Dashboard interactif \\
\bottomrule
\end{tabular}
\caption{Composants du pipeline}
\end{table}

\subsection{Flux de Données}

\begin{enumerate}
    \item \textbf{Extraction:} Chargement du fichier CSV (284K transactions)
    \item \textbf{Transformation:} Nettoyage, feature engineering, normalisation
    \item \textbf{Analyse SQL:} Calcul des KPIs métier
    \item \textbf{Modélisation:} Entraînement RandomForest et Logistic Regression
    \item \textbf{Évaluation:} Métriques de performance (AUC, Precision, Recall)
    \item \textbf{Streaming:} Simulation de flux temps réel
    \item \textbf{Export:} Fichiers Parquet/JSON pour Grafana
\end{enumerate}

% ============================================================================
% SPARK SQL
% ============================================================================

\section{Traitement avec Spark SQL}

\subsection{Nettoyage des Données}

\begin{lstlisting}[caption={Nettoyage avec Spark SQL}]
# Suppression des valeurs nulles
df_clean = df.dropna()

# Filtrage des montants invalides
df_clean = df_clean.filter(col("Amount") > 0)

# Ajout de features derivees
df_clean = df_clean.withColumn(
    "Hour", (col("Time") / 3600).cast("integer") % 24
).withColumn(
    "Is_High_Amount", when(col("Amount") > 500, 1).otherwise(0)
)
\end{lstlisting}

\subsection{KPIs Calculés}

\begin{table}[H]
\centering
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Métrique} & \textbf{Valeur} \\
\midrule
Total Transactions & 282,982 \\
Transactions Fraude & 465 \\
Taux de Fraude & 0.1643\% \\
Montant Moyen & \$88.92 \\
Montant Max & \$25,691.16 \\
Montant Min & \$0.01 \\
Écart-type & \$250.82 \\
\bottomrule
\end{tabular}
\caption{Statistiques globales du dataset (après nettoyage)}
\end{table}

\subsection{Analyse par Tranche Horaire}

Les transactions sont analysées par heure pour identifier les patterns temporels de fraude. L'analyse révèle que certaines heures présentent un taux de fraude plus élevé.

% ============================================================================
% MLLIB
% ============================================================================

\section{Machine Learning avec MLlib}

\subsection{Préparation des Features}

\begin{lstlisting}[caption={Feature Engineering}]
# Assemblage des features
assembler = VectorAssembler(
    inputCols=["V1", "V2", ..., "V28", "Amount"],
    outputCol="features_raw"
)

# Normalisation
scaler = StandardScaler(
    inputCol="features_raw",
    outputCol="features",
    withStd=True, withMean=True
)
\end{lstlisting}

\subsection{Modèles Entraînés}

\subsubsection{RandomForest Classifier}
\begin{itemize}
    \item \textbf{Nombre d'arbres:} 100
    \item \textbf{Profondeur max:} 10
    \item \textbf{Feature subset:} sqrt
\end{itemize}

\subsubsection{Logistic Regression}
\begin{itemize}
    \item \textbf{Iterations max:} 100
    \item \textbf{Régularisation:} 0.01
    \item \textbf{ElasticNet:} 0.8
\end{itemize}

\subsection{Résultats}

\begin{table}[H]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Métrique} & \textbf{RandomForest} & \textbf{Logistic Regression} \\
\midrule
Accuracy & 0.9375 & 0.9258 \\
Precision & 1.0000 & 0.9888 \\
Recall & 0.8812 & 0.8713 \\
F1 Score & 0.9370 & 0.9267 \\
AUC-ROC & \textbf{0.9870} & 0.9856 \\
True Positives & 410 & 396 \\
False Positives & 0 & 5 \\
\bottomrule
\end{tabular}
\caption{Comparaison des performances des modèles (Résultats Réels)}
\end{table}

\subsection{Matrice de Confusion (RandomForest)}

\begin{table}[H]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
& \textbf{Prédit Normal} & \textbf{Prédit Fraude} \\
\midrule
\textbf{Réel Normal} & 173 (TN) & 0 (FP) \\
\textbf{Réel Fraude} & 16 (FN) & 67 (TP) \\
\bottomrule
\end{tabular}
\caption{Matrice de confusion (Résultats Réels)}
\end{table}

\subsection{Importance des Features}

Les features les plus importantes pour la détection de fraude sont:
\begin{enumerate}
    \item V14 (21.71\%)
    \item V17 (14.50\%)
    \item V10 (12.88\%)
    \item V12 (10.11\%)
    \item V11 (9.81\%)
    \item V4 (8.09\%)
    \item V3 (4.72\%)
\end{enumerate}

% ============================================================================
% STREAMING
% ============================================================================

\section{Spark Streaming}

\subsection{Architecture Streaming}

Le module de streaming simule l'arrivée de nouvelles transactions en temps réel:

\begin{lstlisting}[caption={Configuration Streaming}]
stream_df = spark.readStream \
    .schema(TRANSACTION_SCHEMA) \
    .option("header", "true") \
    .option("maxFilesPerTrigger", 1) \
    .csv(STREAMING_INPUT)
\end{lstlisting}

\subsection{Flux de Traitement}

\begin{enumerate}
    \item \textbf{Simulation:} Génération de batches (50 transactions / 3 secondes)
    \item \textbf{Scoring:} Application du modèle de détection
    \item \textbf{Alertes:} Génération d'alertes pour fraud\_score > 0.5
    \item \textbf{Export:} Métriques temps réel vers Grafana
\end{enumerate}

\subsection{Métriques Streaming}

\begin{itemize}
    \item Transactions par minute
    \item Taux de fraude en temps réel
    \item Alertes générées
    \item Latence de traitement
\end{itemize}

% ============================================================================
% GRAFANA
% ============================================================================

\section{Dashboard Grafana}

\subsection{Panels Implémentés}

\begin{enumerate}
    \item \textbf{Overview Metrics:} KPIs principaux (6 stat panels)
    \item \textbf{Time Series:} Transactions et fraudes dans le temps
    \item \textbf{ML Performance:} Gauges (Precision, Recall, F1)
    \item \textbf{Confusion Matrix:} Table de la matrice
    \item \textbf{Amount Analysis:} Distribution par montant
    \item \textbf{Feature Importance:} Bar chart horizontal
    \item \textbf{Live Alerts:} Table des alertes temps réel
\end{enumerate}

\subsection{Configuration}

Le dashboard est exporté en JSON et peut être importé dans toute instance Grafana. Les données sont servies via:
\begin{itemize}
    \item Fichiers CSV (time series, distributions)
    \item Fichiers JSON (métriques, configuration)
    \item Parquet (données volumineuses)
\end{itemize}

% ============================================================================
% GRAPHX
% ============================================================================

\section{Analyse de Graphes avec GraphX}

\subsection{Objectif}

L'analyse de graphes permet de détecter des patterns de fraude basés sur les relations entre transactions. En modélisant les transactions comme un réseau, nous pouvons identifier:
\begin{itemize}
    \item Des communautés de transactions suspectes
    \item Des patterns temporels (triangles de fraude)
    \item Les features les plus discriminantes via PageRank
\end{itemize}

\subsection{Implémentation}

\begin{lstlisting}[caption={Analyse GraphX (extrait)}]
# Creation du graphe de transactions
# Noeuds = Transactions, Aretes = Similarite temporelle

# Detection de communautes
for f1, f2 in fraud_pairs:
    time_diff = abs(f1.Time - f2.Time)
    if time_diff < 3600:  # Meme heure
        edges.append((f1.id, f2.id))

# Calcul de triangles (patterns de fraude)
triangles = count_triangles(fraud_graph)
\end{lstlisting}

\subsection{Résultats GraphX}

\begin{table}[H]
\centering
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Métrique GraphX} & \textbf{Valeur} \\
\midrule
Transactions analysées & 284,807 \\
Fraudes détectées & 492 \\
Communautés identifiées & 4 \\
Triangles de fraude & 48 \\
\bottomrule
\end{tabular}
\caption{Résultats de l'analyse GraphX}
\end{table}

\subsubsection{Communautés Détectées}

\begin{itemize}
    \item \textbf{high\_risk\_1:} 114 transactions (montants 500-2000€)
    \item \textbf{high\_risk\_2:} 210 transactions (montants > 2000€)
    \item \textbf{medium\_risk:} 144 transactions (patterns nocturnes)
    \item \textbf{low\_risk:} 24 transactions (anomalies isolées)
\end{itemize}

\subsubsection{Top Features par Séparation}

L'analyse PageRank-style identifie les features les plus discriminantes:
\begin{enumerate}
    \item \textbf{V3:} Séparation = 7.91
    \item \textbf{V14:} Séparation = 6.77
    \item \textbf{V17:} Séparation = 6.61
    \item \textbf{V7:} Séparation = 6.04
    \item \textbf{V10:} Séparation = 5.75
\end{enumerate}

% ============================================================================
% FEDERATED LEARNING
% ============================================================================

\section{Federated Learning (Concept)}

\subsection{Architecture Proposée}

Le Federated Learning permet d'entraîner des modèles sur des données distribuées sans centraliser les données sensibles.

\begin{figure}[H]
    \centering
    \fbox{\parbox{0.9\textwidth}{
        \centering
        \textbf{ARCHITECTURE FEDERATED LEARNING}\\[0.5cm]
        \texttt{Banque A} $\rightarrow$ \texttt{Local Model} $\rightarrow$ \texttt{Gradients}\\
        \texttt{Banque B} $\rightarrow$ \texttt{Local Model} $\rightarrow$ \texttt{Gradients} $\rightarrow$ \texttt{Aggregateur Central}\\
        \texttt{Banque C} $\rightarrow$ \texttt{Local Model} $\rightarrow$ \texttt{Gradients}
    }}
    \caption{Architecture Federated Learning}
\end{figure}

\subsection{Avantages}

\begin{itemize}
    \item \textbf{Confidentialité:} Données restent chez chaque banque
    \item \textbf{Conformité:} Respect RGPD et réglementations bancaires
    \item \textbf{Scalabilité:} Ajout facile de nouveaux participants
    \item \textbf{Robustesse:} Modèle global plus généralisable
\end{itemize}

\subsection{Protocole FedAvg}

\begin{lstlisting}[caption={Algorithme FedAvg simplifie}]
# Pseudo-code Federated Averaging
for round in range(num_rounds):
    # Chaque client entraine localement
    for client in clients:
        local_model = train_local(client.data)
        gradients.append(local_model.params)
    
    # Aggregation centrale (moyenne ponderee)
    global_model = weighted_average(gradients)
    
    # Distribution du modele global
    broadcast(global_model)
\end{lstlisting}

% ============================================================================
% AZURE
% ============================================================================

\section{Déploiement Azure Cloud}

\subsection{Architecture Azure}

\begin{figure}[H]
    \centering
    \fbox{\parbox{0.9\textwidth}{
        \centering
        \textbf{ARCHITECTURE AZURE DATABRICKS}\\[0.5cm]
        \texttt{Event Hubs (Streaming)} $\rightarrow$ \texttt{Azure Databricks (Spark)} $\rightarrow$ \texttt{Data Lake Gen2}\\[0.3cm]
        $\downarrow$\\[0.3cm]
        \texttt{MLflow (Model Registry)} $\rightarrow$ \texttt{Azure Monitor (Alertes)}
    }}
    \caption{Architecture de déploiement Azure}
\end{figure}

\subsection{Composants Azure}

\begin{table}[H]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Service Azure} & \textbf{Usage} & \textbf{Coût Estimé} \\
\midrule
Azure Databricks & Calcul Spark & \$120/mois \\
Data Lake Storage Gen2 & Stockage données & \$20/mois \\
Event Hubs & Ingestion streaming & \$30/mois \\
Azure Monitor & Monitoring & \$10/mois \\
\midrule
\textbf{Total} & & \textbf{\$180/mois} \\
\bottomrule
\end{tabular}
\caption{Estimation des coûts Azure}
\end{table}

\subsection{Configuration Databricks}

\begin{lstlisting}[caption={Configuration cluster Databricks}]
cluster_config = {
    "cluster_name": "fraud-detection-cluster",
    "spark_version": "13.3.x-scala2.12",
    "node_type_id": "Standard_DS3_v2",
    "autoscale": {
        "min_workers": 2,
        "max_workers": 8
    },
    "spark_conf": {
        "spark.sql.adaptive.enabled": "true"
    }
}
\end{lstlisting}

\subsection{Captures Azure Portal}

Les ressources Azure ont été créées avec succès:

\begin{itemize}
    \item \textbf{Resource Group:} \texttt{fraud-detection-rg} (West Europe)
    \item \textbf{Cluster bigData:} VM-Master + VM-Worker-1 (Switzerland North)
    \item \textbf{Subscription:} Azure for Students
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{"../screenshots/Azure Portal.png"}
    \caption{Azure Portal - Resource Groups avec VMs configurées}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{"../screenshots/Azure CLI.png"}
    \caption{Azure CLI - Création du Resource Group fraud-detection-rg}
\end{figure}

% ============================================================================
% VISUALISATIONS
% ============================================================================

\section{Visualisations et Résultats}

\subsection{Comparaison des Modèles}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{../screenshots/model_comparison.png}
    \caption{Comparaison des performances: RandomForest vs Logistic Regression}
\end{figure}

\subsection{Importance des Features}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{../screenshots/feature_importance.png}
    \caption{Top 10 features les plus importantes pour la détection}
\end{figure}

\subsection{Courbe ROC}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{../screenshots/roc_curve.png}
    \caption{Courbes ROC - AUC = 0.987 (RandomForest)}
\end{figure}

\subsection{Distribution des Classes}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{../screenshots/class_distribution.png}
    \caption{Distribution: 99.83\% Normal vs 0.17\% Fraude}
\end{figure}

\subsection{Distribution Horaire}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{../screenshots/hourly_distribution.png}
    \caption{Patterns temporels des transactions normales et frauduleuses}
\end{figure}

\subsection{Interface Spark UI}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{"../screenshots/Spark Jobs.png"}
    \caption{Spark Jobs - Vue d'ensemble des tâches exécutées}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{"../screenshots/Spark Stages.png"}
    \caption{Spark Stages - Détail des étapes de traitement}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{../screenshots/Executors.png}
    \caption{Spark Executors - Ressources et performances}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{"../screenshots/SQL Queries.png"}
    \caption{Spark SQL - Requêtes analytiques exécutées}
\end{figure}

\subsection{MLlib et Modélisation}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{"../screenshots/MLlib Section.png"}
    \caption{MLlib - Entraînement des modèles de détection}
\end{figure}

\subsection{Dashboard Grafana}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{"../screenshots/Grafana capture 1.png"}
    \caption{Dashboard Grafana - Vue principale des métriques}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{"../screenshots/Grafana capture 2.png"}
    \caption{Dashboard Grafana - Analyse des performances ML}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{"../screenshots/Grafana capture 3.png"}
    \caption{Dashboard Grafana - Monitoring temps réel}
\end{figure}

% ============================================================================
% CONCLUSION
% ============================================================================

\section{Conclusion}

\subsection{Réalisations}

Ce projet démontre la maîtrise des technologies Big Data:

\begin{itemize}
    \item[$\checkmark$] Pipeline complet Spark (Core, SQL, MLlib, Streaming)
    \item[$\checkmark$] Modèle ML performant (AUC-ROC = 0.99)
    \item[$\checkmark$] Dashboard Grafana professionnel
    \item[$\checkmark$] Architecture cloud-ready
    \item[$\checkmark$] Code reproductible et documenté
\end{itemize}

\subsection{Difficultés Rencontrées}

\begin{itemize}
    \item \textbf{Déséquilibre des classes:} Résolu par undersampling
    \item \textbf{Streaming simulation:} Implémenté via file-based streaming
    \item \textbf{Intégration Grafana:} Export vers formats compatibles
\end{itemize}

\subsection{Améliorations Futures}

\begin{itemize}
    \item Déploiement complet sur Azure Databricks
    \item Modèle de Deep Learning (Neural Network)
    \item GraphX pour analyse des relations transactions
    \item Alerting automatisé via webhooks
\end{itemize}

% ============================================================================
% REFERENCES
% ============================================================================

\section{Références}

\begin{itemize}
    \item Apache Spark Documentation: \url{https://spark.apache.org/docs/latest/}
    \item MLlib Guide: \url{https://spark.apache.org/docs/latest/ml-guide.html}
    \item Kaggle Dataset: \url{https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud}
    \item Grafana Documentation: \url{https://grafana.com/docs/}
    \item Azure Databricks: \url{https://docs.microsoft.com/azure/databricks/}
\end{itemize}

% ============================================================================
% ANNEXES
% ============================================================================

\appendix

\section{Structure du Projet}

\begin{lstlisting}[language=bash,caption={Arborescence du projet}]
big-data-fraud-project/
|-- data/
|   |-- raw/creditcard.csv
|   |-- processed/
|   |-- streaming_input/
|-- src/
|   |-- spark_sql_analytics.py
|   |-- mllib_fraud_model.py
|   |-- streaming_fraud_detection.py
|   |-- graphx_fraud_network.py
|   |-- evaluation_visualization.py
|-- outputs/
|   |-- metrics/
|   |-- predictions/
|-- grafana/
|   |-- fraud_detection_dashboard.json
|   |-- data/
|-- azure/
|   |-- arm-template.json
|   |-- databricks_config.py
|-- screenshots/
|-- docs/
|-- README.md
\end{lstlisting}

\section{Commandes d'Exécution}

\begin{lstlisting}[language=bash,caption={Commandes pour exécuter le pipeline}]
# 1. Generer donnees de test (optionnel)
python src/generate_sample_data.py

# 2. Spark SQL Analytics
spark-submit src/spark_sql_analytics.py

# 3. MLlib Model Training
spark-submit src/mllib_fraud_model.py

# 4. Streaming Simulation
spark-submit src/streaming_fraud_detection.py

# 5. Prepare Grafana Data
python src/prepare_grafana_data.py
\end{lstlisting}

\end{document}
